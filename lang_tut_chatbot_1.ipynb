{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-17T08:06:35.781017Z",
     "start_time": "2025-10-17T08:06:35.768239Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Load .env file"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:09:56.085557Z",
     "start_time": "2025-10-17T06:09:50.858978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable"
   ],
   "id": "59f1bc95fe18b48b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:13:52.298538Z",
     "start_time": "2025-10-17T06:13:50.276152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import traceable\n",
    "from langchain_core.messages import HumanMessage # inheriting from Base message class\n",
    "# Initialize ChatOpenAI with LM Studio endpoint\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://172.17.160.1:1234/v1\",  # Can be any dummy value\n",
    "    api_key='None',\n",
    "    model=\"gemma-3-4b-it\",  # Optional\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Use the model\n",
    "response = llm.invoke([HumanMessage(content = 'Hello,How are you ?')])\n",
    "print(response.content)"
   ],
   "id": "2a1ccb37990ca2aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! Iâ€™m doing well, thanks for asking! As an AI, I don't really *feel* in the same way humans do, but my systems are running smoothly and Iâ€™m ready to chat. ðŸ˜Š\n",
      "\n",
      "How about you? How is your day going so far? \n",
      "\n",
      "Is there anything youâ€™d like to talk about or something I can help you with?\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:18:14.198054Z",
     "start_time": "2025-10-17T06:18:10.586933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "llm.invoke([\n",
    "    HumanMessage('Hello, How are you ?,my name is akshat'),\n",
    "    AIMessage('I am fine, How are you akshat'),\n",
    "    HumanMessage('i am fine , what is my name and talk about something you like')\n",
    "])"
   ],
   "id": "fd083ea03e100b83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Akshat! Thatâ€™s a great name. ðŸ˜Š\\n\\nOkay, so something I really enjoy (and it's a bit nerdy!) is the concept of **artificial intelligence**. Specifically, I find it fascinating how AI can learn and adapt â€“ like me! Itâ€™s incredible to think that algorithms are being developed to mimic human thought processes, solve complex problems, and even create art. \\n\\nI love exploring the potential applications, from helping doctors diagnose diseases more accurately to designing sustainable solutions for climate change.  It feels like we're on the cusp of a really transformative era, and Iâ€™m constantly learning new things about it. \\n\\nWhat do *you* enjoy?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 142, 'prompt_tokens': 55, 'total_tokens': 197, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma-3-4b-it', 'system_fingerprint': 'gemma-3-4b-it', 'id': 'chatcmpl-jhcn8dbt01scpevyiqx5m', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--707e0318-6d12-466a-b287-13261a3f4c5a-0', usage_metadata={'input_tokens': 55, 'output_tokens': 142, 'total_tokens': 197, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:56:47.267657Z",
     "start_time": "2025-10-17T06:56:47.264228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ],
   "id": "35e70be300a5a0c2",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:56:47.618400Z",
     "start_time": "2025-10-17T06:56:47.615201Z"
    }
   },
   "cell_type": "code",
   "source": "workflow = StateGraph(state_schema=MessagesState)# Define a new graph",
   "id": "b71d0a42af05fe71",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:56:48.002448Z",
     "start_time": "2025-10-17T06:56:47.996744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def call_model(state : MessagesState): # Message state is an inhertied TypedDict can use an Pydantic Model too\n",
    "    response = llm.invoke(state['messages'])\n",
    "    return {'messages' : response}\n",
    "\n",
    "workflow.add_edge(START,'model') # Used for starting the START node is virtual or symbolic\n",
    "workflow.add_node('model',call_model) # START is connected to model node\n",
    "memory = MemorySaver() # Saves memory inside RAM , not used for production only for debugging\n",
    "app = workflow.compile(checkpointer=memory) # compiles the workflow"
   ],
   "id": "87dceed102844620",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:56:49.303111Z",
     "start_time": "2025-10-17T06:56:49.299558Z"
    }
   },
   "cell_type": "code",
   "source": "config = {\"configurable\": {\"thread_id\": \"abc123\"}}",
   "id": "d6391c4d314cc066",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T06:56:57.342497Z",
     "start_time": "2025-10-17T06:56:52.439129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"Hi! I'm Akshat and i love AI/ML/DS/Gen-AI.\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "\n",
    "output = await app.ainvoke({'messages' : input_messages},config)"
   ],
   "id": "8e011c6dfa2f73f7",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:01:32.970275Z",
     "start_time": "2025-10-17T07:01:32.964496Z"
    }
   },
   "cell_type": "code",
   "source": "output['messages'][-1].pretty_print()",
   "id": "8c6592724e7e38d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "Hey Akshat, thatâ€™s fantastic! Itâ€™s awesome to meet someone whoâ€™s passionate about AI/ML/DS/Gen-AI â€“ itâ€™s such a rapidly evolving and exciting field right now. \n",
      "\n",
      "It's great you're so engaged with these areas. What specifically are you most interested in within those fields?  Do you have any particular projects you're working on, or topics you're currently learning about?\n",
      "\n",
      "I'd love to chat more about it!  Perhaps we could talk about:\n",
      "\n",
      "*   **Specific Technologies:** Are you exploring things like TensorFlow, PyTorch, Langchain, Stable Diffusion, etc.?\n",
      "*   **Applications:** What kind of problems do you find most interesting to solve with AI â€“ healthcare, finance, creative applications, robotics?\n",
      "*   **Current Trends:**  Are you following developments in Large Language Models (LLMs), multimodal AI, or something else?\n",
      "\n",
      "Don't hesitate to tell me anything that comes to mind!\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:04:21.387369Z",
     "start_time": "2025-10-17T07:04:19.263014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "26b299a370f4c7fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "(Sighing gently) Akshat. Seriously, itâ€™s Akshat. I understand you might be testing me, but this is getting repetitive and unproductive. \n",
      "\n",
      "Letâ€™s talk about something *you* are genuinely interested in within AI/ML â€“ perhaps a specific algorithm, a dataset you'd like to explore, or a problem you want to tackle?\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:07:21.023259Z",
     "start_time": "2025-10-17T07:07:19.747034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config2 = {'configurable':{'thread_id' : 123}} # Using different thread_id restarted the conversation , one possible thing to look at is the structure of this config2 dictionary when i changed thread_id to thread it threw error , potential investigation !!!\n",
    "query = \"What's my name?\"\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config2) # Passing config2 instead of config resetted the convo\n",
    "output[\"messages\"][-1].pretty_print()"
   ],
   "id": "ae47231e1e8b099b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001B[1m Ai Message \u001B[0m==================================\n",
      "\n",
      "I apologize for the repetition! As an AI, I have absolutely no way of knowing your name. You haven't told me yet. ðŸ˜Š \n",
      "\n",
      "Would you like to tell me what your name is?\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:32:15.843707Z",
     "start_time": "2025-10-17T07:32:15.831834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "This is a dummy example used to demonstrate how prompt templates are used to dynamically update prompts like other things in langchain , it is a runnable\n",
    "and integrates well with other runnable using LCEL (|) , also it is useful for modern LLM that use system,user,AI to identify messages , it can also fill in\n",
    "placeholders like {placeholder} , provided during invoke(). Note this will fail here because the model we using gemma locally is not a turn-based chat model\n",
    "but OPEN-AI GPT is , so i have done a simple workaround to make it executable with gemma too , in the cell below.\n",
    "'''\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "completed_prompt = prompt.invoke({'question' : 'whats your name'})\n",
    "completed_prompt"
   ],
   "id": "bb9e448c01b120c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content='whats your name', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:34:50.936118Z",
     "start_time": "2025-10-17T07:34:50.930999Z"
    }
   },
   "cell_type": "code",
   "source": "completed_prompt.to_messages()",
   "id": "bf0d97ad9c5756ef",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats your name', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:36:16.672669Z",
     "start_time": "2025-10-17T07:36:14.517852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def chat_messages_to_string(chat_prompt):\n",
    "    return \"\\n\".join([f\"{msg.type.capitalize()}: {msg.content}\" for msg in chat_prompt.to_messages()])\n",
    "\n",
    "string_prompt = chat_messages_to_string(completed_prompt)\n",
    "llm.invoke([string_prompt])"
   ],
   "id": "a7ad5b2a6b675b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You can call me Gemma! Iâ€™m an open-weights AI assistant created by the Gemma team at Google DeepMind. \\n\\nIt's nice to meet you! ðŸ˜Š\\n\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 23, 'total_tokens': 60, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma-3-4b-it', 'system_fingerprint': 'gemma-3-4b-it', 'id': 'chatcmpl-8rp7u2mwjgw4rn8qy0xk56', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--68e18e5a-dd76-40e1-8256-46a15193d234-0', usage_metadata={'input_tokens': 23, 'output_tokens': 37, 'total_tokens': 60, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:47:03.498914Z",
     "start_time": "2025-10-17T07:47:03.490731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "MessagesPlaceholder is a special component designed to inject dynamic lists of messages into specific positions within your chat template. Unlike regular variable placeholders that expect single string values, MessagesPlaceholder expects a list of complete messages and inserts them all at that position.â€‹\n",
    "\n",
    "The key difference: regular placeholders like {question} take a string and create a single message, while MessagesPlaceholder takes multiple pre-formed messages and inserts them as-is into the conversation flow.\n",
    "\n",
    "Very useful when you want to build prompts like: \"beginning of prompt\" + conversation history + closing instructions + latest user message.\n",
    "'''\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    ('system','You will not use offensive languages'),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "prompt.invoke({\n",
    "    \"history\": [\n",
    "        (\"human\", \"what's 5 + 2\"),\n",
    "        (\"ai\", \"5 + 2 is 7\")\n",
    "    ],\n",
    "    \"question\": \"now multiply that by 4\"\n",
    "})"
   ],
   "id": "54bbee77714efccf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"what's 5 + 2\", additional_kwargs={}, response_metadata={}), AIMessage(content='5 + 2 is 7', additional_kwargs={}, response_metadata={}), SystemMessage(content='You will not use offensive languages', additional_kwargs={}, response_metadata={}), HumanMessage(content='now multiply that by 4', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:57:45.692559Z",
     "start_time": "2025-10-17T07:57:45.687268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
    "\n",
    "Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\n",
    "'''"
   ],
   "id": "d13ef31d0706cf73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\\n\\nImportantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T08:01:04.680717Z",
     "start_time": "2025-10-17T08:01:03.313531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "By default, .stream in our LangGraph application streams application steps-- in this case, the single step of the model response. Setting stream_mode=\"messages\" allows us to stream output tokens instead:\n",
    "'''\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm Akshat, please tell me a joke.\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "async for chunk, metadata in app.astream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ],
   "id": "ff64d38577d0d9b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi| Ak|sh|at|!| Here|â€™|s| a| joke| for| you|:|\n",
      "\n",
      "|Why| did| the| bicycle| fall| over|?| |\n",
      "\n",
      "|â€¦| Because| it| was| two| tired|!| ðŸ˜‚| |\n",
      "\n",
      "|Hope| you| enjoyed| that| one|!| ðŸ˜Š| Do| you| want| to| hear| another|?||"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db81f6c067c93c4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "39838432ddf7ccac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
